<!DOCTYPE html>
<html lang="en-US">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>FleSpeech</title>
  <meta name="generator" content="Jekyll v3.9.0">
  <meta property="og:title" content="FleSpeech">
  <meta property="og:locale" content="en_US">

  <meta name="twitter:card" content="summary">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <link rel="stylesheet" href="style.css">
</head>

<body data-new-gr-c-s-check-loaded="14.1001.0" data-gr-ext-installed="">
  <section class="page-header">
    <h1 id="">
      <center>FleSpeech: Flexibly Controllable Speech Generation with Multimodal Prompt</center>
    </h1>
  </section>

  <section class="main-content">
    
    <center><img src='fig/241212_intro.drawio.png' style="width: auto;height: auto;max-width: 60%;"></center>

    <br><br>

    <div align="left"><b>Fig.1</b> FlexiSpeech can flexibly generate speech that matches user-provided prompts. It offers various flexible usage methods, including but not limited to: 1) providing two speech samples that represent different speaking styles and tones; 2) using a facial image to synthesize matching sounds, which can be further supplemented with natural language descriptions; 3) specifying the desired sound solely through text descriptions; and 4) providing an audio clip and using text descriptions to further control attributes such as speech rate and intonation.</div>

    <br><br>

    <h1 id="abstract" style="text-align: center;">Abstract</h1>
    <p> Large-scale speech synthesis systems enable high realism. There is a growing emphasis on flexible and controllable speech synthesis. Existing controllable speech generation methods typically accept only single or fixed prompts, lacking creativity and flexibility. These limitations make it challenging to meet specific user needs in certain scenarios, such as adjusting the style while preserving the selected speaker's timbre or choosing a style and generating a voice that matches a character's appearance. To address these challenges, we propose \textit{FleSpeech}, a novel multi-stage speech generation framework that integrates various forms of control, allowing for more flexible manipulation of speech attributes. FleSpeech employs a multimodal prompt encoder that can process and unify different text, audio, and visual prompts into a cohesive representation. This approach enhances speech synthesis's adaptability and supports creative and precise control over the generated speech. Additionally, we have developed a data collection pipeline for multimodal datasets to support further research and applications in this field. Comprehensive subjective and objective experiments demonstrate the effectiveness of FleSpeech.
    </p>

    <br><br>

    <center><img src='fig/241212_model.drawio.png' style="width: auto;height: auto;max-width: 100%;"></center>
    <div align="center"><b>Fig.2</b> The architecture of FleSpeech.</div>

    <br><br>

    <h1 id="Results" style="text-align: center;">Audio samples</h1>

    <h2 id="Results">Single-Prompt Controllable TTS</h2>

    <h2 id="Results">Multi-Prompt Controllable TTS</h2>

    <h2 id="Results">Extensibility</h2>
      
        
    <footer class="site-footer">

      <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com/">GitHub
          Pages</a>.</span>
    </footer>
  </section>
</body>

</html>
